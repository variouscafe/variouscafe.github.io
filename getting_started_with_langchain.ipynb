{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/variouscafe/variouscafe.github.io/blob/master/getting_started_with_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VdKDqIJGxXo",
        "outputId": "e5d5f85a-9d30-49b8-c024-5dc14795e2d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.13-py3-none-any.whl (810 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.29 (from langchain)\n",
            "  Downloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.33 (from langchain)\n",
            "  Downloading langchain_core-0.1.34-py3-none-any.whl (271 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.33-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.33->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.13 langchain-community-0.0.29 langchain-core-0.1.34 langchain-text-splitters-0.0.1 langsmith-0.1.33 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.14.3-py3-none-any.whl (262 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.9/262.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.3\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP4vp6xXpFrN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# 코랩사용자인 경우 == >\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# < == 코랩사용자인 경우\n",
        "\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM 모델 추론해보기\n",
        "\n",
        "안녕 > LLM > 하세요"
      ],
      "metadata": {
        "id": "Nsfm0p4nqldU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCyakdVyKbgp",
        "outputId": "162892c4-c89b-4e1b-f433-91e6311e88f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "하세요, 제 이름은 {}입니다.'.format(\"채은지\"))\n",
            "\n",
            "# 숫자 포매팅\n",
            "print('저는 올해 {}살입니다.'.format(25))\n",
            "\n",
            "# 여러 개의 값을 넣기\n",
            "print('제가 좋아하는 숫자는 {}와 {}입니다.'.format(3, 7))\n",
            "\n",
            "# 인덱스와 값을 혼용해서 넣기\n",
            "print('제가 좋아하는 숫자는 {1}와 {0}입니다.'.format(3, 7))\n",
            "\n",
            "# 이름으로 넣기\n",
            "print('저는 {name}입니다. 제 나이는 {age}살입니다.'.format(age=25, name=\"채은지\"))\n",
            "\n",
            "# f-string\n",
            "name = \"채은지\"\n",
            "print(f\"안녕하세요, 제 이름은 {name}입니다.\") # f-string은 문자열 앞에 f를 붙여 사용합니다.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI() # 키는 환경설정에서 가지고 옴\n",
        "\n",
        "prompt = \"안녕\"\n",
        "\n",
        "# llm 대규모언어모델 텍스트를 입력받아 텍스트를 출력\n",
        "# chatllm은 편지봉투 + 편지지라면,\n",
        "# llm은 엽서와 같다.\n",
        "\n",
        "# predict는 머신러닝/딥러닝에서 추론할 때 사용하는 함수명\n",
        "response = llm.predict(prompt)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat LLM 모델 추론해보기\n",
        "\n",
        "안녕 > Chat LLM > 안녕하세요! 무엇을 도와드릴까요?"
      ],
      "metadata": {
        "id": "tX44fyMHqr_v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJNtEBKGWKRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2b15a8-f1b9-4aa0-b486-08f084fc4d8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요! 무엇을 도와드릴까요?\n"
          ]
        }
      ],
      "source": [
        "# 챗모델\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI()\n",
        "\n",
        "prompt = \"안녕\"\n",
        "\n",
        "response = chat_model.predict(prompt)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP9_6mw7DpTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148f653e-b96e-40fa-e36c-2af07d79bbcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "response = chat_model.predict(\"hello~\")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat_model.predict(\"오하이요 고자이마스\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL68YB4bq5Lt",
        "outputId": "b371b15c-6123-4a9b-9527-9a6bd8901085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요! 고자이마스! (こんにちは！ありがとうございます！) 혹시 무언가 도와드릴 것이 있나요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 메시지 구성하기 (편지봉투 + 편지지)\n",
        "\n",
        "- SystemMessage\n",
        "- HumanMessage\n",
        "- AIMessage"
      ],
      "metadata": {
        "id": "Xy1mo8VZrEZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbFSOfilHefs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f684dbd-95e3-455d-fca5-4ab2b92cedc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict_messages` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Rainbow Threads' response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 22, 'total_tokens': 25}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat_model = ChatOpenAI()\n",
        "\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "\n",
        "# 편지지 1장을 담아 편지봉투\n",
        "messages = [HumanMessage(content=text)]\n",
        "\n",
        "response = chat_model.predict_messages(messages)\n",
        "\n",
        "print(response) # AIMessage을 얻는다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkx-PvvCL7Nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd208183-1c22-4751-ec95-646d77422dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='나는 프로그래밍을 좋아해요.' response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 26, 'total_tokens': 40}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat_model = ChatOpenAI(temperature=0.9)\n",
        "\n",
        "# 편지봉투 1통, 편지지 2장\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to Korean.\"), # 개발자\n",
        "    HumanMessage(content=\"I love programming.\") # 사용자 입력\n",
        "]\n",
        "\n",
        "response = chat_model(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프롬프트 템플릿"
      ],
      "metadata": {
        "id": "uEKrLwIZIv8A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzctRSCoXY8i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c785cb-1f6d-4af2-efae-694d2bf5e3a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='나는 프로그래밍을 좋아해요.' response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 26, 'total_tokens': 40}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# 템플릿은 양식이다.\n",
        "# 양식 가정 : 여러번 사용할 때, 효율적으로 하기 위함\n",
        "# 여러 번 사용한다는 의미는 값이 바뀌는 거랑, 고정된 것 구분할 수 있어야 합니다.\n",
        "# 모두 고정이 되어 있다. > 양식을 사용할 필요가 없고\n",
        "# 모두 변경된다. > 양식을 사용할 필요가 없다.\n",
        "\n",
        "#템플릿은 변수를 포함있다. 변수 {}\n",
        "system_content_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "\n",
        "# role = 'system'\n",
        "# SystemMessagePromptTemplate > SystemMessage(role=systme)_PromptTemplate(템플식)\n",
        "system_message_prompt_template = SystemMessagePromptTemplate.from_template(system_content_template)\n",
        "\n",
        "human_content_template=\"{text}\"\n",
        "\n",
        "human_message_prompt_template = HumanMessagePromptTemplate.from_template(human_content_template)\n",
        "\n",
        "# 편지지가 2장 각각 편지지가 양식\n",
        "# 우리가 채워야할 변수\n",
        "# 3개 {input_language}, {output_language}, {text}\n",
        "# system 양식 {input_language}, {output_language}\n",
        "# user 양식 {text}\n",
        "# 은행 계좌 만들거나, 계약\n",
        "\n",
        "# 채팅용 프롬프트 템플릿 > 양식(편지지)이 담긴 편지봉투\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt_template,\n",
        "                                                         human_message_prompt_template])\n",
        "\n",
        "chat_model = ChatOpenAI(temperature=0)\n",
        "\n",
        "# variable > prompt_template > prompt_value(=prompt) > chat_model\n",
        "\n",
        "messages = chat_prompt_template.format_prompt(\n",
        "                            input_language=\"English\",\n",
        "                            output_language=\"Korean\",\n",
        "                            text=\"I love programming.\").to_messages()\n",
        "\n",
        "response =  chat_model(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SezYDsX6XWvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c265f53-8ce4-41bd-c882-bfc4fcab7d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='나는 프로그래밍을 사랑해요.' response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 26, 'total_tokens': 40}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts.chat import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "system_content_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "human_content_template = \"{text}\"\n",
        "\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", system_content_template),\n",
        "    (\"human\", human_content_template),\n",
        "])\n",
        "\n",
        "messages = chat_prompt_template.format_messages(input_language=\"English\",\n",
        "                                                output_language=\"Korean\",\n",
        "                                                text=\"I love programming.\")\n",
        "\n",
        "chat_model = ChatOpenAI(temperature=0.9)\n",
        "\n",
        "# 미리 양식 작성, 현장에서 작성\n",
        "\n",
        "response = chat_model(messages)\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OutputParser"
      ],
      "metadata": {
        "id": "9mXSXL7lKer4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2tjQBn9XiCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc4006f6-2b42-43b8-85e3-7334fc28fa62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'bye']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from langchain.schema import BaseOutputParser\n",
        "\n",
        "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
        "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
        "\n",
        "    def parse(self, text: str):\n",
        "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
        "        return text.strip().split(\", \")\n",
        "\n",
        "CommaSeparatedListOutputParser().parse(\"hi, bye\")\n",
        "# >> ['hi', 'bye']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지는 프롬프트를 구성해서 llm 모델을 직접 호출\n",
        "\n",
        "# LLMChain"
      ],
      "metadata": {
        "id": "e0Xvup2iKztI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD7bTVVNXG03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "aa5b9b46-6ed5-4cdf-854d-431517ba7913"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'저는 프로그래밍을 좋아합니다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "chat_model = ChatOpenAI(temperature=0)\n",
        "\n",
        "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt,\n",
        "                                                         human_message_prompt])\n",
        "# 기존에는 아래와 같이 호출\n",
        "# chat_model.predict_message(chat_prompt_template)\n",
        "# chat_model(chat_prompt_template)\n",
        "\n",
        "# chat_model에 직접 호출하지 않고 체인을 이용하는 방식\n",
        "\n",
        "# 체인을 구성합니다.\n",
        "chain = LLMChain(llm = chat_model,\n",
        "                 prompt = chat_prompt_template)\n",
        "\n",
        "# 체인을 호출합니다. 호출 시 필요한 변수를 넣습니다.\n",
        "chain.run(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO7YNxAPX56j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "bb23690e-e34a-4be3-9556-ad3e62946eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py:57: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m```text\n",
            "13**0.3432\n",
            "```\n",
            "...numexpr.evaluate(\"13**0.3432\")...\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLLM 프롬프트 예상\\n\\n사용자가 입력한 문장을 numexpr.evaluate에 적합하도록 작성합니다.\\n예시\\n입력 : \"What is 13 raised to the .3432 power?\"\\n출력 :\\n```text\\n13**0.3432\\n```\\n\\n출력양식\\n```text\\nnumexpr.evaluate에 넣을 인자값\\n```\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from langchain import OpenAI, LLMMathChain\n",
        "\n",
        "llm = OpenAI(temperature = 0)\n",
        "\n",
        "llm_math = LLMMathChain(llm = llm,\n",
        "                        verbose=True)\n",
        "\n",
        "llm_math.invoke(\"What is 13 raised to the .3432 power?\")\n",
        "\n",
        "'''\n",
        "LLM 프롬프트 예상\n",
        "\n",
        "사용자가 입력한 문장을 numexpr.evaluate에 적합하도록 작성합니다.\n",
        "예시\n",
        "입력 : \"What is 13 raised to the .3432 power?\"\n",
        "출력 :\n",
        "```text\n",
        "13**0.3432\n",
        "```\n",
        "\n",
        "출력양식\n",
        "```text\n",
        "numexpr.evaluate에 넣을 인자값\n",
        "```\n",
        "'''\n",
        "\n",
        "# LLM의 출력을 파싱해서 그 값을 아래 함수의 인자로 호출합니다.\n",
        "# numexpr.evaluate('13**0.3432')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BiK22aRBvCBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}